{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    # valid_ratio是验证集比例，我们通过这个比例来划分训练集和验证集\n",
    "    #一整个的数据集就是一个data_set，我们用他的长度乘以valid_ratio来规定验证集的长度\n",
    "    #为了避免出现小数，我们用int()来取整,并保存在valid_set_size中\n",
    "    valid_set_size = int(valid_ratio * len(data_set))\n",
    "    #计算训练集大小\n",
    "    train_set_size = len(data_set)-valid_set_size\n",
    "    #我们用随机数来作为划分训练集和验证集的比例\n",
    "    train_set, valid_set = random_split(data_set,[train_set_size, valid_set_size],generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader, model, device):\n",
    "    #将模型设置为评估模式，这会关闭dropout和batch normalization\n",
    "    model.eval()\n",
    "    preds=[]\n",
    "    for x in tqdm(test_loader):# 使用tqdm显示进度条\n",
    "        x = x.to(device) # 将输入数据移动到device\n",
    "        with torch.no_grad():# 禁用梯度运算\n",
    "            pred = model(x) # 获取模型对输入数据的预测\n",
    "            preds.append(pred.detach().cpu()) # 将预测结果从计算图中分离出来，转移到 CPU 上，并将其添加到 preds 列表中\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVID19Dataset(Dataset):\n",
    "    '''\n",
    "    x: Features.\n",
    "    y: Targets, if none, do prediction.\n",
    "    '''\n",
    "    def __init__(self, x, y=None):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_dim):# input_dim是输入数据的特征维度\n",
    "        super(My_Model, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),# 输入层\n",
    "            nn.ReLU(),# 激活函数\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.layers(x) # 前向传播\n",
    "        x = x.squeeze(1) # 去掉最外层的维度 (B,1) -> (B)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 17 Best feature score \n",
      "[876214.8322447  335591.6630519  311015.24307534 205326.08451419\n",
      " 181409.55205618 139190.43353702  17703.25449295  17532.56653353\n",
      "  17360.80858132  17153.20445066  17005.71942696  16802.10300361\n",
      "  16289.94077877  16133.22580844  11089.09711071  11031.55168611\n",
      "  10775.62521955]\n",
      "\n",
      "Top 17 Best feature index \n",
      "[72 56 73 57 40 41 87 86 71 70 55 54 39 38 85 69 53]\n",
      "\n",
      "Top 17 Best feature name\n",
      "Index(['hh_cmnty_cli.2', 'hh_cmnty_cli.1', 'nohh_cmnty_cli.2',\n",
      "       'nohh_cmnty_cli.1', 'hh_cmnty_cli', 'nohh_cmnty_cli', 'ili.3', 'cli.3',\n",
      "       'ili.2', 'cli.2', 'ili.1', 'cli.1', 'ili', 'cli', 'tested_positive.2',\n",
      "       'tested_positive.1', 'tested_positive'],\n",
      "      dtype='object')\n",
      "[np.int64(38), np.int64(39), np.int64(40), np.int64(41), np.int64(53), np.int64(54), np.int64(55), np.int64(56), np.int64(57), np.int64(69), np.int64(70), np.int64(71), np.int64(72), np.int64(73), np.int64(85), np.int64(86), np.int64(87)]\n",
      "Index(['cli', 'ili', 'hh_cmnty_cli', 'nohh_cmnty_cli', 'tested_positive',\n",
      "       'cli.1', 'ili.1', 'hh_cmnty_cli.1', 'nohh_cmnty_cli.1',\n",
      "       'tested_positive.1', 'cli.2', 'ili.2', 'hh_cmnty_cli.2',\n",
      "       'nohh_cmnty_cli.2', 'tested_positive.2', 'cli.3', 'ili.3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "train_data = pd.read_csv('./covid.train.csv')\n",
    "train_data.head()\n",
    "# iloc[]函数，属于pandas库，即对数据进行位置索引，从而在数据表中提取出相应的数据。\n",
    "x_data, y_data= train_data.iloc[:, 0:88], train_data.iloc[:, 88]\n",
    "\n",
    "k = 17 # 选择k个相关性最强的特征，是超参之一\n",
    "# 调用sklearn中的SelectKBest函数\n",
    "'''\n",
    "  SelectKBest:\n",
    "    score_func: 评估指标，可选值包括：卡方检验（chi2）、互信息（mutual_info_classif、mutual_info_regression）等，默认为卡方检验。\n",
    "    k: 选择排名靠前的k个特征，默认为10。\n",
    "\n",
    "'''\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "result = selector.fit(x_data, y_data)\n",
    "\n",
    "# result.scores_ includes scores for each features\n",
    "# np.argsort sort scores in ascending order by index, we reverse it to make it descending.\n",
    "idx = np.argsort(result.scores_)[::-1] # 每个feature关于score_func的倒序排序的index\n",
    "print(f'Top {k} Best feature score ')\n",
    "print(result.scores_[idx[:k]]) # 选取前k个特征\n",
    "\n",
    "print(f'\\nTop {k} Best feature index ')\n",
    "print(idx[:k])\n",
    "\n",
    "print(f'\\nTop {k} Best feature name')\n",
    "print(x_data.columns[idx[:k]])\n",
    "\n",
    "selected_idx = list(np.sort(idx[:k]))\n",
    "print(selected_idx)\n",
    "print(x_data.columns[selected_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feat(train_data, valid_data, test_data, select_all=True):\n",
    "    y_train, y_valid = train_data[:,-1], valid_data[:,-1] # 获取标签\n",
    "    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data # 获取特征\n",
    "    if select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1])) # 选择所有特征\n",
    "    else:\n",
    "        feat_idx = [0,1,2,3,4] # 选择部分特征\n",
    "    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "    # 使用均方误差作为损失函数, reduction一共有三个可选值\n",
    "    #none表示不进行任何缩减，返回每个样本的损失；'mean'表示计算所有样本损失的的均值；sum表示返回所有样本的损失之和\n",
    "    criterion = nn.MSELoss(reduction = 'mean')\n",
    "    # optimizer是优化器实例，将用于更新模型的参数以最小化损失函数\n",
    "    # SGD是pytorch提供的随机梯度下降优化器； model.parameters()会返回模型中所有需要优化的参数\n",
    "    # lr是学习率，是一个超参数，这里使用config中的学习率\n",
    "    # momentum是动量因子，用于加速 SGD 优化器收敛，动量项可以帮助优化器在损失函数的凹槽中稳定下来，避免震荡，并且在陡峭方向上加速收敛\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n",
    "    # Writer是一个用于记录训练过程的类，./model是record的保存路径\n",
    "    Writer = SummaryWriter()\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models')\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # 将模型设置为训练模式\n",
    "        loss_record=[]\n",
    "        # 使用tqdm显示进度条\n",
    "        train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        for x, y in train_pbar:\n",
    "            optimizer.zero_grad() # set gradient to zero\n",
    "            x, y = x.to(device), y.to(device) # 将输入数据和标签移动到device\n",
    "            pred = model(x) # 获取模型对输入数据的预测\n",
    "            loss = criterion(pred, y) # 计算损失\n",
    "            loss.backward()# 反向传播\n",
    "            optimizer.step() # 更新参数,执行一步优化, (参数 = 参数 - 学习率 * 参数的梯度), 优化器会自动更新模型的参数\n",
    "            step +=1\n",
    "            loss_record.append(loss.detach().item())# 将损失记录到loss_record中,detach()是为了防止内存泄漏,因为loss是一个计算图,我们只需要数值,item()是将tensor转换为python数值\n",
    "            # epoch是当前的轮次，n_epochs是总的轮次\n",
    "            # loss是当前轮次的损失\n",
    "            # set_description是设置进度条的描述\n",
    "            # set_postfix是设置进度条的后缀\n",
    "            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "        writer.add_scalar('Loss/train',mean_train_loss, step)\n",
    "\n",
    "        model.eval() # set your model to evaluation mode\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:# 从验证数据集中批量加载数据\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "            loss_record.append(loss.item())\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss:{mean_valid_loss:.4f}')\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path'])# save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0 # reset early_stop_count, because we find a better model\n",
    "        else:\n",
    "                early_stop_count += 1\n",
    "        \n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 5201314,\n",
    "    'select_all': True,# whether to use all features\n",
    "    'valid_ratio': 0.2,\n",
    "    'n_epochs': 3000,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-5,\n",
    "    'early_stop': 400, #if model has not been improved for 400 epochs, we will stop training\n",
    "    'save_path': './models/model.ckpt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: (2160, 118)\n",
      "valid_data size: (539, 118)\n",
      "test_data size: (1078, 117)\n",
      "number of features: 117\n"
     ]
    }
   ],
   "source": [
    "same_seed(config['seed'])\n",
    "train_data, test_data = pd.read_csv('./covid.train.csv').values, pd.read_csv('./covid.test.csv').values\n",
    "train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n",
    "\n",
    "#print out the data size\n",
    "print(f\"\"\"train_data size: {train_data.shape}\n",
    "valid_data size: {valid_data.shape}\n",
    "test_data size: {test_data.shape}\"\"\")\n",
    "\n",
    "#select features\n",
    "x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['select_all'])\n",
    "\n",
    "#print out the number of features\n",
    "print(f'number of features: {x_train.shape[1]}')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train),COVID19Dataset(x_valid, y_valid), COVID19Dataset(x_test)\n",
    "\n",
    "# pytorch data loader loads pytorch dataset into batches\n",
    "#shuffle=True means that the data will be shuffled before each epoch,shuffle即打乱顺序\n",
    "#pin_memory=True means that the data loader will copy Tensors into CUDA pinned memory before returning them\n",
    "#即：在将数据传递给 GPU 进行计算之前，首先将它们复制到固定内存中，以提高数据传输速度。\n",
    "#固定内存是一种特殊类型的主机内存，它可以更快地与 GPU 内存进行数据交换。将数据复制到固定内存中可以加速从主机内存到 GPU 内存的数据传输。\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m My_Model(input_dim\u001b[38;5;241m=\u001b[39mx_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(train_loader, valid_loader, model, config, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(x) \u001b[38;5;66;03m# 获取模型对输入数据的预测\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, y) \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# 更新参数,执行一步优化, (参数 = 参数 - 学习率 * 参数的梯度), 优化器会自动更新模型的参数\u001b[39;00m\n\u001b[1;32m     27\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "model = My_Model(input_dim=x_train.shape[1]).to(device)\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
